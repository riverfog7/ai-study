{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-06T16:01:49.779065Z",
     "start_time": "2025-10-06T16:01:45.900910Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List\n",
    "import torch\n",
    "\n",
    "\n",
    "from attr import dataclass\n",
    "\n",
    "sys.path.append('./minigpt4')\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from transformers import StoppingCriteriaList\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.dist_utils import get_rank\n",
    "from minigpt4.common.registry import registry\n",
    "from minigpt4.conversation.conversation import Chat, CONV_VISION_Vicuna0, CONV_VISION_LLama2, StoppingCriteriaSub\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riverfog7/Workspace/AISogang/.venv/lib/python3.12/site-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riverfog7/Workspace/AISogang/.venv/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/Users/riverfog7/Workspace/AISogang/.venv/lib/python3.12/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T16:01:49.784841Z",
     "start_time": "2025-10-06T16:01:49.782629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    cfg_path: str = './minigpt4/eval_configs/minigpt4_eval.yaml'\n",
    "    options: list = None\n",
    "    gpu_id: int = 0\n",
    "\n",
    "def setup_seeds(config):\n",
    "    seed = config.run_cfg.seed + get_rank()\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True"
   ],
   "id": "57315278830380e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T16:01:49.797186Z",
     "start_time": "2025-10-06T16:01:49.793953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MiniGPT4Wrapper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg_path: str = './minigpt4/eval_configs/minigpt4_eval.yaml',\n",
    "        device: str = None,\n",
    "        gpu_id: int = 0\n",
    "    ):\n",
    "        if device is None:\n",
    "            self.device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        if self.device == 'cuda':\n",
    "            self.device = f'cuda:{gpu_id}'\n",
    "\n",
    "        args = Args(cfg_path=cfg_path, gpu_id=gpu_id)\n",
    "        cfg = Config(args)\n",
    "\n",
    "        model_config = cfg.model_cfg\n",
    "        model_cls = registry.get_model_class(model_config.arch)\n",
    "        self.model = model_cls.from_config(model_config).to(self.device)\n",
    "\n",
    "        self.model.visual_encoder.float()\n",
    "        self.model.ln_vision.float()\n",
    "        self.model.eval()\n",
    "\n",
    "        original_forward = self.model.llama_model.forward\n",
    "        def forward_wrapper(*args, **kwargs):\n",
    "            kwargs.pop('cache_position', None)\n",
    "            return original_forward(*args, **kwargs)\n",
    "        self.model.llama_model.forward = forward_wrapper\n",
    "\n",
    "        if hasattr(self.model.llama_model, 'generation_config'):\n",
    "            self.model.llama_model.generation_config.do_sample = False\n",
    "\n",
    "        conv_dict = {\n",
    "            'pretrain_vicuna0': CONV_VISION_Vicuna0,\n",
    "            'pretrain_llama2': CONV_VISION_LLama2\n",
    "        }\n",
    "        self.conv_template = conv_dict[model_config.model_type]\n",
    "\n",
    "        vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "        self.vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "\n",
    "        stop_words_ids = [[835], [2277, 29937]]\n",
    "        stop_words_ids = [torch.tensor(ids).to(device=self.device) for ids in stop_words_ids]\n",
    "        self.stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "\n",
    "        self.chat = Chat(\n",
    "            self.model,\n",
    "            self.vis_processor,\n",
    "            device=self.device,\n",
    "            stopping_criteria=self.stopping_criteria\n",
    "        )\n",
    "\n",
    "        self.chat_state = None\n",
    "        self.img_list = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.chat_state = self.conv_template.copy()\n",
    "        self.img_list = []\n",
    "\n",
    "    def set_image(self, image_path: str):\n",
    "        self.chat_state = self.conv_template.copy()\n",
    "        self.img_list = []\n",
    "        self.chat.upload_img(image_path, self.chat_state, self.img_list)\n",
    "        self.chat.encode_img(self.img_list)\n",
    "\n",
    "    def ask(self, message: str):\n",
    "        self.chat.ask(message, self.chat_state)\n",
    "        llm_message, _ = self.chat.answer(\n",
    "            conv=self.chat_state,\n",
    "            img_list=self.img_list,\n",
    "            num_beams=1,\n",
    "            temperature=1.0,\n",
    "            max_new_tokens=300,\n",
    "            max_length=2000\n",
    "        )\n",
    "        return llm_message"
   ],
   "id": "5e266130f528c0d9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T16:02:17.303799Z",
     "start_time": "2025-10-06T16:01:49.800649Z"
    }
   },
   "cell_type": "code",
   "source": "wrapper = MiniGPT4Wrapper(cfg_path='./minigpt4/eval_configs/minigpt4_eval.yaml', device=device)",
   "id": "ddaa133e2fb5105a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "/Users/riverfog7/Workspace/AISogang/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5853b5a846ba42508ebdcbac55faa2b1"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Q-Former\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Q-Former Done\n",
      "Load MiniGPT-4 Checkpoint: weights/pretrained_minigpt4.pth\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T16:02:19.613799Z",
     "start_time": "2025-10-06T16:02:17.342944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img = Image.open('../AudioCLIP/dataset/italian pasta recipe/0/frames/video_0.jpg')\n",
    "wrapper.set_image('../AudioCLIP/dataset/italian pasta recipe/0/frames/video_0.jpg')\n",
    "wrapper.ask(\"Describe the image in detail.\")"
   ],
   "id": "8f4f300adc8a108",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riverfog7/Workspace/AISogang/MiniGPT-4/minigpt4/minigpt4/models/base_model.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast(dtype=dtype)\n",
      "/Users/riverfog7/Workspace/AISogang/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/riverfog7/Workspace/AISogang/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m img = Image.open(\u001B[33m'\u001B[39m\u001B[33m../AudioCLIP/dataset/italian pasta recipe/0/frames/video_0.jpg\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      2\u001B[39m wrapper.set_image(\u001B[33m'\u001B[39m\u001B[33m../AudioCLIP/dataset/italian pasta recipe/0/frames/video_0.jpg\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[43mwrapper\u001B[49m\u001B[43m.\u001B[49m\u001B[43mask\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mDescribe the image in detail.\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 71\u001B[39m, in \u001B[36mMiniGPT4Wrapper.ask\u001B[39m\u001B[34m(self, message)\u001B[39m\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mask\u001B[39m(\u001B[38;5;28mself\u001B[39m, message: \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m     70\u001B[39m     \u001B[38;5;28mself\u001B[39m.chat.ask(message, \u001B[38;5;28mself\u001B[39m.chat_state)\n\u001B[32m---> \u001B[39m\u001B[32m71\u001B[39m     llm_message, _ = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mchat\u001B[49m\u001B[43m.\u001B[49m\u001B[43manswer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     72\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconv\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mchat_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimg_list\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mimg_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_beams\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     75\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     76\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m300\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     77\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2000\u001B[39;49m\n\u001B[32m     78\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     79\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m llm_message\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Workspace/AISogang/MiniGPT-4/minigpt4/minigpt4/conversation/conversation.py:187\u001B[39m, in \u001B[36mChat.answer\u001B[39m\u001B[34m(self, conv, img_list, **kargs)\u001B[39m\n\u001B[32m    185\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34manswer\u001B[39m(\u001B[38;5;28mself\u001B[39m, conv, img_list, **kargs):\n\u001B[32m    186\u001B[39m     generation_dict = \u001B[38;5;28mself\u001B[39m.answer_prepare(conv, img_list, **kargs)\n\u001B[32m--> \u001B[39m\u001B[32m187\u001B[39m     output_token = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel_generate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgeneration_dict\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n\u001B[32m    188\u001B[39m     output_text = \u001B[38;5;28mself\u001B[39m.model.llama_tokenizer.decode(output_token, skip_special_tokens=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    190\u001B[39m     output_text = output_text.split(\u001B[33m'\u001B[39m\u001B[33m###\u001B[39m\u001B[33m'\u001B[39m)[\u001B[32m0\u001B[39m]  \u001B[38;5;66;03m# remove the stop sign '###'\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Workspace/AISogang/MiniGPT-4/minigpt4/minigpt4/conversation/conversation.py:207\u001B[39m, in \u001B[36mChat.model_generate\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    204\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmodel_generate\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m    205\u001B[39m     \u001B[38;5;66;03m# for 8 bit and 16 bit compatibility\u001B[39;00m\n\u001B[32m    206\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.model.maybe_autocast():\n\u001B[32m--> \u001B[39m\u001B[32m207\u001B[39m         output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mllama_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    208\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Workspace/AISogang/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Workspace/AISogang/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2252\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[39m\n\u001B[32m   2244\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2245\u001B[39m         input_ids=input_ids,\n\u001B[32m   2246\u001B[39m         expand_size=generation_config.num_return_sequences,\n\u001B[32m   2247\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2248\u001B[39m         **model_kwargs,\n\u001B[32m   2249\u001B[39m     )\n\u001B[32m   2251\u001B[39m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2252\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2253\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2254\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2255\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2256\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2257\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2258\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2259\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2260\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2262\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001B[32m   2263\u001B[39m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[32m   2264\u001B[39m     beam_scorer = BeamSearchScorer(\n\u001B[32m   2265\u001B[39m         batch_size=batch_size,\n\u001B[32m   2266\u001B[39m         num_beams=generation_config.num_beams,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2271\u001B[39m         max_length=generation_config.max_length,\n\u001B[32m   2272\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Workspace/AISogang/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3297\u001B[39m, in \u001B[36mGenerationMixin._sample\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[39m\n\u001B[32m   3295\u001B[39m     probs = nn.functional.softmax(next_token_scores, dim=-\u001B[32m1\u001B[39m)\n\u001B[32m   3296\u001B[39m     \u001B[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3297\u001B[39m     next_tokens = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmultinomial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m.squeeze(\u001B[32m1\u001B[39m)\n\u001B[32m   3298\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3299\u001B[39m     next_tokens = torch.argmax(next_token_scores, dim=-\u001B[32m1\u001B[39m)\n",
      "\u001B[31mRuntimeError\u001B[39m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c22734cad8a90d24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8a82d1fce952eae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a1acea43dee1ff50",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
