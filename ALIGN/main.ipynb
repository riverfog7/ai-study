{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c67dbd2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Optional, Dict, Any\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchview import draw_graph\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "from plotly_tools import *\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AlignModel, AlignProcessor, AlignTextConfig, AlignVisionConfig\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"../.hf_home\"\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "03a2a00a",
   "metadata": {},
   "source": [
    "### Load Model & Processor"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8952c0fc03a8ff",
   "metadata": {},
   "source": [
    "model = AlignModel.from_pretrained(\"kakaobrain/align-base\", cache_dir=os.environ[\"HF_HOME\"])\n",
    "processor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\", cache_dir=os.environ[\"HF_HOME\"])\n",
    "config_vision = AlignVisionConfig.from_pretrained(\"kakaobrain/align-base\", cache_dir=os.environ[\"HF_HOME\"])\n",
    "config_text = AlignTextConfig.from_pretrained(\"kakaobrain/align-base\", cache_dir=os.environ[\"HF_HOME\"])\n",
    "\n",
    "text_model = model.text_model\n",
    "vision_model = model.vision_model\n",
    "text_projection = model.text_projection"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf6ecbbf",
   "metadata": {},
   "source": [
    "#### Switch model to eval mode"
   ]
  },
  {
   "cell_type": "code",
   "id": "76cbeb8e0bd8d81",
   "metadata": {},
   "source": [
    "_ = model.eval()\n",
    "_ = text_model.eval()\n",
    "_ = vision_model.eval()\n",
    "_ = text_projection.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "03f4c67c",
   "metadata": {},
   "source": [
    "### Model architecture exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31dea71",
   "metadata": {},
   "source": [
    "#### Image Size & Text Length"
   ]
  },
  {
   "cell_type": "code",
   "id": "50e4b1a8",
   "metadata": {},
   "source": [
    "text_model.config.max_position_embeddings, processor.tokenizer.model_max_length, vision_model.config.image_size"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2549dbab",
   "metadata": {},
   "source": [
    "#### Tokenizer / Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "id": "da3c2d50",
   "metadata": {},
   "source": [
    "processor.tokenizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8404942b",
   "metadata": {},
   "source": [
    "target_sentence = \"a photo of a cat\"\n",
    "processor.tokenizer.tokenize(target_sentence)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3c2f4e6",
   "metadata": {},
   "source": [
    "cut_len = 15\n",
    "tokenized = processor.tokenizer(target_sentence, return_tensors=\"pt\", padding=\"max_length\")\n",
    "\n",
    "print(f\"\"\"Input IDs: \\t\\t{torch.flatten(tokenized.input_ids).tolist()[:cut_len]}\n",
    "Attention Mask: \\t{torch.flatten(tokenized.attention_mask).tolist()[:cut_len]}\n",
    "Token Type IDs: \\t{torch.flatten(tokenized.token_type_ids).tolist()[:cut_len]}\n",
    "Total Length: \\t\\t{tokenized.input_ids.shape[1]}\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "161c85db",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    text_model_out = text_model(**tokenized)\n",
    "text_model_out.pooler_output.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85ffebb6",
   "metadata": {},
   "source": [
    "text_embedding = text_projection(text_model_out.pooler_output)\n",
    "text_embedding.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ecaa73f7",
   "metadata": {},
   "source": [
    "#### Image Processing & Image Embedding"
   ]
  },
  {
   "cell_type": "code",
   "id": "5a01300b",
   "metadata": {},
   "source": [
    "image = Image.open(\"sample_images/cat.jpg\").convert(\"RGB\")\n",
    "image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a500c5f3",
   "metadata": {},
   "source": [
    "processor.image_processor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "df12bc8f",
   "metadata": {},
   "source": [
    "processed_image = processor.image_processor(images=image, return_tensors=\"pt\")\n",
    "display(Image.fromarray(np.uint8(processed_image.pixel_values[0].permute(1, 2, 0).numpy() * 255)))\n",
    "processed_image.pixel_values.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a06b8f5a",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    vision_model_out = vision_model(**processed_image)\n",
    "vision_embedding = vision_model_out.pooler_output\n",
    "vision_embedding.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2dd3bedb",
   "metadata": {},
   "source": [
    "#### Model Pipeline Visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "26b4321850dfc4bd",
   "metadata": {},
   "source": [
    "graph = draw_graph(model, input_size=[(1,64), (1,3,289,289)], dtypes=[torch.long, torch.float32], expand_nested=True)\n",
    "graph.visual_graph"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9fa2719d",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "c6176f50",
   "metadata": {},
   "source": [
    "def embed_image_batch(image: List[Image.Image]) -> List[torch.Tensor]:\n",
    "    model.to(device)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=\"max_length\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_embeddings = model.get_image_features(**inputs)\n",
    "    return image_embeddings.cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "def embed_text_batch(text: List[str]) -> List[torch.Tensor]:\n",
    "    model.to(device)\n",
    "    inputs = processor(text=text, return_tensors=\"pt\", padding=\"max_length\").to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = model.get_text_features(**inputs)\n",
    "    return text_embeddings.cpu().numpy().tolist()\n",
    "\n",
    "def load_images(paths: List[Path]) -> List[Image.Image]:\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {path}: {e}\")\n",
    "    return images\n",
    "\n",
    "def normalize_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4e14084a",
   "metadata": {},
   "source": [
    "### Run Inference on Custom Images and Captions"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa50b022",
   "metadata": {},
   "source": [
    "inference_needed = not os.path.exists(\"./processed_data.parquet\")\n",
    "print(f\"Running inference: {inference_needed}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2eca698b",
   "metadata": {},
   "source": [
    "images_dir = \"sample_images\"\n",
    "if inference_needed:\n",
    "    df_lst = []\n",
    "    for directory in os.listdir(images_dir):\n",
    "        if not os.path.isdir(os.path.join(images_dir, directory)):\n",
    "            continue\n",
    "\n",
    "        img_cls = directory\n",
    "        rel_path = os.path.join(images_dir, directory)\n",
    "\n",
    "        img_paths = [os.path.join(rel_path, img_name) for img_name in os.listdir(rel_path) if not '.' in img_name]\n",
    "        img_paths.sort()\n",
    "\n",
    "        df_lst.append(pd.DataFrame({\n",
    "            \"image_path\": img_paths,\n",
    "            \"class\": [img_cls] * len(img_paths),\n",
    "            \"image_embedding\": [None] * len(img_paths),\n",
    "            \"text_embedding\": [None] * len(img_paths),\n",
    "        }))\n",
    "\n",
    "    df_embeddings = pd.concat(df_lst, ignore_index=True)\n",
    "    del df_lst"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eaf8ecfe",
   "metadata": {},
   "source": [
    "batch_size = 128\n",
    "if inference_needed:\n",
    "    unique_class = df_embeddings['class'].unique().tolist()\n",
    "    class_embedding_mapping = {v: e for v, e in zip(unique_class, embed_text_batch(unique_class))}\n",
    "    df_embeddings['text_embedding'] = df_embeddings['class'].map(class_embedding_mapping)\n",
    "\n",
    "    img_embeddings = []\n",
    "    for i in tqdm(range(0, len(df_embeddings), batch_size)):\n",
    "        batch_df = df_embeddings.iloc[i:i+batch_size]\n",
    "        batch_images = load_images(batch_df['image_path'].tolist())\n",
    "        batch_embeddings = embed_image_batch(batch_images)\n",
    "        img_embeddings.extend(batch_embeddings)\n",
    "        del batch_images, batch_embeddings, batch_df\n",
    "        torch.mps.empty_cache()\n",
    "    df_embeddings['image_embedding'] = img_embeddings\n",
    "    del img_embeddings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a75a2bfb",
   "metadata": {},
   "source": [
    "### Save / Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "75063bca",
   "metadata": {},
   "source": [
    "if inference_needed:\n",
    "    df_embeddings.to_parquet(\"processed_data.parquet\", index=False, compression=\"brotli\", engine=\"fastparquet\")\n",
    "else:\n",
    "    df_embeddings = pd.read_parquet(\"processed_data.parquet\", engine=\"fastparquet\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "797bb40a",
   "metadata": {},
   "source": [
    "### Visualize Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591ac20",
   "metadata": {},
   "source": [
    "#### Get Image"
   ]
  },
  {
   "cell_type": "code",
   "id": "4621aac6",
   "metadata": {},
   "source": [
    "df_embeddings['image'] = df_embeddings['image_path'].apply(lambda x: Image.open(x).convert(\"RGB\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84fd16ff",
   "metadata": {},
   "source": [
    "#### Normalize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "30fc1390",
   "metadata": {},
   "source": [
    "df_embeddings['image_embedding_normalized'] = normalize_embeddings(np.asarray(df_embeddings['image_embedding'].tolist())).tolist()\n",
    "df_embeddings['text_embedding_normalized'] = normalize_embeddings(np.asarray(df_embeddings['text_embedding'].tolist())).tolist()\n",
    "df_embeddings.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "033c9224",
   "metadata": {},
   "source": [
    "#### Compute Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d6ea83",
   "metadata": {},
   "source": [
    "##### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "id": "28960f4b",
   "metadata": {},
   "source": [
    "tnse = TSNE(n_components=3, random_state=random_seed, init='random', learning_rate='auto', perplexity=5)\n",
    "results = tnse.fit_transform(np.asarray(df_embeddings['image_embedding_normalized'].tolist() + df_embeddings['text_embedding_normalized'].tolist()))\n",
    "\n",
    "img_proj = results[:len(df_embeddings['image_embedding_normalized'])]\n",
    "txt_proj = results[len(df_embeddings['image_embedding_normalized']):]\n",
    "\n",
    "df_embeddings['tnse-image-3d'] = list(img_proj)\n",
    "df_embeddings['tnse-text-3d'] = list(txt_proj)\n",
    "\n",
    "df_embeddings['tnse-image-3d-x'] = df_embeddings['tnse-image-3d'].apply(lambda x: x[0])\n",
    "df_embeddings['tnse-image-3d-y'] = df_embeddings['tnse-image-3d'].apply(lambda x: x[1])\n",
    "df_embeddings['tnse-image-3d-z'] = df_embeddings['tnse-image-3d'].apply(lambda x: x[2])\n",
    "\n",
    "df_embeddings['tnse-text-3d-x'] = df_embeddings['tnse-text-3d'].apply(lambda x: x[0])\n",
    "df_embeddings['tnse-text-3d-y'] = df_embeddings['tnse-text-3d'].apply(lambda x: x[1])\n",
    "df_embeddings['tnse-text-3d-z'] = df_embeddings['tnse-text-3d'].apply(lambda x: x[2])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b6a6873a",
   "metadata": {},
   "source": [
    "##### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec9fbb10",
   "metadata": {},
   "source": [
    "umap = umap.UMAP(n_neighbors=5, min_dist=0.3, n_components=3, random_state=random_seed)\n",
    "\n",
    "results = umap.fit_transform(np.asarray(df_embeddings['image_embedding_normalized'].tolist() + df_embeddings['text_embedding_normalized'].tolist()))\n",
    "\n",
    "img_proj = results[:len(df_embeddings['image_embedding_normalized'])]\n",
    "txt_proj = results[len(df_embeddings['image_embedding_normalized']):]\n",
    "\n",
    "df_embeddings['umap-image-3d'] = list(img_proj)\n",
    "df_embeddings['umap-text-3d'] = list(txt_proj)\n",
    "\n",
    "df_embeddings['umap-image-3d-x'] = df_embeddings['umap-image-3d'].apply(lambda x: x[0])\n",
    "df_embeddings['umap-image-3d-y'] = df_embeddings['umap-image-3d'].apply(lambda x: x[1])\n",
    "df_embeddings['umap-image-3d-z'] = df_embeddings['umap-image-3d'].apply(lambda x: x[2])\n",
    "\n",
    "df_embeddings['umap-text-3d-x'] = df_embeddings['umap-text-3d'].apply(lambda x: x[0])\n",
    "df_embeddings['umap-text-3d-y'] = df_embeddings['umap-text-3d'].apply(lambda x: x[1])\n",
    "df_embeddings['umap-text-3d-z'] = df_embeddings['umap-text-3d'].apply(lambda x: x[2])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1440d49e",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d24cf9",
   "metadata": {},
   "source": [
    "##### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "id": "417727ae",
   "metadata": {},
   "source": [
    "save_3d_hover_images_html(\n",
    "    \"embeddings_tsne.html\",\n",
    "    df_embeddings['tnse-image-3d'].tolist(),\n",
    "    df_embeddings['tnse-text-3d'].tolist(),\n",
    "    df_embeddings['image'].tolist(),\n",
    "    df_embeddings['class'].tolist(),\n",
    "    df_embeddings['class'].tolist(),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a24116e",
   "metadata": {},
   "source": [
    "##### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "id": "777324ee",
   "metadata": {},
   "source": [
    "save_3d_hover_images_html(\n",
    "    \"embeddings_umap.html\",\n",
    "    df_embeddings['umap-image-3d'].tolist(),\n",
    "    df_embeddings['umap-text-3d'].tolist(),\n",
    "    df_embeddings['image'].tolist(),\n",
    "    df_embeddings['class'].tolist(),\n",
    "    df_embeddings['class'].tolist(),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ad0c5c7",
   "metadata": {},
   "source": [
    "##### Embedding Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "id": "6e675b19",
   "metadata": {},
   "source": [
    "text_class = df_embeddings['class'].unique().tolist()\n",
    "text_class_embeddings = normalize_embeddings(embed_text_batch(text_class)).tolist()\n",
    "\n",
    "image_class = df_embeddings['class'].tolist()\n",
    "image_class_embeddings = df_embeddings['image_embedding_normalized'].tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed06aa67",
   "metadata": {},
   "source": [
    "use_cpu = False\n",
    "\n",
    "taget_device = \"cpu\" if use_cpu else device\n",
    "img_e = torch.as_tensor(image_class_embeddings, dtype=torch.float32, device=taget_device)\n",
    "txt_e = torch.as_tensor(text_class_embeddings, dtype=torch.float32, device=taget_device)\n",
    "\n",
    "sim = img_e @ txt_e.T\n",
    "sim = torch.nn.Softmax(dim=-1)(sim)\n",
    "sim = sim.cpu().numpy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8edf1577",
   "metadata": {},
   "source": [
    "# visualize similarity matrix as a plot with plotly\n",
    "fig = px.imshow(sim, \n",
    "                labels=dict(x=\"Text Class\", y=\"Image Class\", color=\"Similarity\"),\n",
    "                x=text_class,\n",
    "                y=image_class,\n",
    "                color_continuous_scale='Viridis')\n",
    "fig.update_xaxes(side=\"top\")\n",
    "fig.update_layout(width=1100, height=720)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "227ce44b7dcbd0a6",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aisogang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
