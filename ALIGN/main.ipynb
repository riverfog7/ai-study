{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c67dbd2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T09:11:24.242366Z",
     "start_time": "2025-09-23T09:11:24.235922Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchview import draw_graph\n",
    "import torch\n",
    "from transformers import AlignModel, AlignProcessor\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"../.hf_home\"\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a2a00a",
   "metadata": {},
   "source": [
    "### Load Model & Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adbda982f245e76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T08:23:00.096901Z",
     "start_time": "2025-09-23T08:22:54.101745Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AlignModel.from_pretrained(\"kakaobrain/align-base\", cache_dir=os.environ[\"HF_HOME\"])\n",
    "processor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\", cache_dir=os.environ[\"HF_HOME\"])\n",
    "text_model = model.text_model\n",
    "vision_model = model.vision_model\n",
    "text_projection = model.text_projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ecbbf",
   "metadata": {},
   "source": [
    "#### Switch model to eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbeb8e0bd8d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T08:20:48.697267Z",
     "start_time": "2025-09-23T08:20:48.690400Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = model.eval()\n",
    "_ = text_model.eval()\n",
    "_ = vision_model.eval()\n",
    "_ = text_projection.eval()\n",
    "\n",
    "text_projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4c67c",
   "metadata": {},
   "source": [
    "### Model architecture exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31dea71",
   "metadata": {},
   "source": [
    "#### Image Size & Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e4b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.config.max_position_embeddings, processor.tokenizer.model_max_length, vision_model.config.image_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549dbab",
   "metadata": {},
   "source": [
    "#### Tokenizer / Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = \"a photo of a cat\"\n",
    "processor.tokenizer.tokenize(target_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_len = 15\n",
    "tokenized = processor.tokenizer(target_sentence, return_tensors=\"pt\", padding=\"max_length\")\n",
    "\n",
    "print(f\"\"\"Input IDs: \\t\\t{torch.flatten(tokenized.input_ids).tolist()[:cut_len]}\n",
    "Attention Mask: \\t{torch.flatten(tokenized.attention_mask).tolist()[:cut_len]}\n",
    "Token Type IDs: \\t{torch.flatten(tokenized.token_type_ids).tolist()[:cut_len]}\n",
    "Total Length: \\t\\t{tokenized.input_ids.shape[1]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c85db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    text_model_out = text_model(**tokenized)\n",
    "text_model_out.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = text_projection(text_model_out.pooler_output)\n",
    "text_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa73f7",
   "metadata": {},
   "source": [
    "#### Image Processing & Image Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a01300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"sample_images/cat.jpg\").convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a500c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df12bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_image = processor.image_processor(images=image, return_tensors=\"pt\")\n",
    "display(Image.fromarray(np.uint8(processed_image.pixel_values[0].permute(1, 2, 0).numpy() * 255)))\n",
    "processed_image.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vision_model_out = vision_model(**processed_image)\n",
    "vision_embedding = vision_model_out.pooler_output\n",
    "vision_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd3bedb",
   "metadata": {},
   "source": [
    "#### Model Pipeline Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4321850dfc4bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T09:13:21.160807Z",
     "start_time": "2025-09-23T09:13:18.943703Z"
    }
   },
   "outputs": [],
   "source": [
    "graph = draw_graph(model, input_size=[(1,64), (1,3,289,289)], dtypes=[torch.long, torch.float32], expand_nested=True)\n",
    "graph.visual_graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aisogang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
